{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dafVtLBGqEfH"
      },
      "source": [
        "# Lab 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_UQAINeqEfL"
      },
      "source": [
        "### <span style=\"color:chocolate\"> Submission requirements </span>\n",
        "\n",
        "Your homework will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n",
        "\n",
        "Additional points may be deducted if these requirements are not met:\n",
        "    \n",
        "* Comment your code\n",
        "* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own\n",
        "* Try and minimize the use of the global namespace (meaning, keep things inside functions)\n",
        "* Upload your .ipynb file to Gradescope when done\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w17v819qEfM"
      },
      "source": [
        "#### ``Objectives``\n",
        "1. Implement a CNN to detect diabetic retinopathy (DR) from retina images taken using fundus photography under a variety of imaging conditions\n",
        "2. Improve generalization performance and reduce overfitting using **image transformation** and **data augmentation** techniques\n",
        "3. Tune hyperparameters of the CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV-nBWdJqEfM"
      },
      "source": [
        "#### ``Motivation``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRxZ2EQIqEfM"
      },
      "source": [
        "Diabetic retinopathy (DR) is an eye condition that  affects blood vessels in the retina. It can cause vision loss and blindness in people who have diabetes. Screening for DR allows earlier and more effective treatment options for millions of people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI3q32SCqEfN"
      },
      "source": [
        "#### ``Data``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3sGygJQqEfN"
      },
      "source": [
        "In this assignment you will use a small dataset of retina images (`Download` links: [images](https://drive.google.com/drive/folders/1sdfUC64Un1iwuiHEehcbijxB54OhU_nd?usp=sharing) and [labels](https://drive.google.com/drive/folders/1MOlSJBZg7L1HtG5vHPt77ighRvQaGfDg?usp=sharing)). You will **build** and **train** a **CNN model** to predict whether or not to refer a patient for DR treatment using binarized severity of DR in patients: no referral if {No DR, mild} and referral if {moderate, severe, and proliferate DR}.\n",
        "\n",
        "\n",
        "<u>Note</u>: the original dataset is hosted by Kaggle [[Source]](https://www.kaggle.com/c/aptos2019-blindness-detection/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc2Q9bodsHNw",
        "outputId": "adaddbbb-3c12-4ce0-9f0b-4ea932602acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1co-mxR0qEfO"
      },
      "source": [
        "Import the necessary libraries and make sure to replace IMAGE_PATH and LABEL_PATH with the path to the directories where you saved the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OBeUyW3qEfO"
      },
      "outputs": [],
      "source": [
        "# standard\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# tf and keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# plots\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(2)\n",
        "%matplotlib inline\n",
        "\n",
        "# FILL IN CODE HERE #\n",
        "IMAGE_PATH = '' # replace with your path\n",
        "LABEL_PATH = '' # replace with your path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgFc391jqEfQ"
      },
      "source": [
        "At this point, you may ask yourself what the best CNN model that fits this data is. First, you will want to read through the data description in Kaggle (see the link to the original dataset above). Understanding what you are working with challenges you to write preprocessing code that uncovers your data's most helpful information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcqG4Zq_qEfQ"
      },
      "source": [
        "---\n",
        "### Exercise 1 (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU9KgLqiqEfQ"
      },
      "source": [
        "Read the data description from Kaggle and list (a) the source of images and (b) the labeling procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQXpL0OKqEfQ"
      },
      "source": [
        "a. The images were captured using fundus photography from various clinics, utilizing different cameras and imaging conditions.\n",
        "\n",
        "b. The labeling procedure involves a clinician rating each image for the severity of diabetic retinopathy on a scale ranging from 0 to 4. The severity levels are defined as follows:  0 for No DR, 1 for Mild, 2 for Moderate, 3 for Severe, and 4 for Proliferative DR. These severity ratings are assigned by clinicians who assess the images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Tk47UuqEfQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8qNNFvfqEfR"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "874zZLRoqEfR"
      },
      "source": [
        "Let's now explore our dataset. We will start with label inspection and continue with image visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "IfWaQoi8qEfR",
        "outputId": "c837ca28-7684-4bc7-91d6-742566da5a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of labels: (200, 2)\n",
            "Unique diagnosis codes: [0 1 2 3 4]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3ef95c17-52e4-46fe-98d1-3a81b9c5be3e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_code</th>\n",
              "      <th>diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000c1434d8d7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0024cdab0c1e</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0083ee8054ee</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00a8624548a9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00b74780d31d</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ef95c17-52e4-46fe-98d1-3a81b9c5be3e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ef95c17-52e4-46fe-98d1-3a81b9c5be3e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ef95c17-52e4-46fe-98d1-3a81b9c5be3e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3b9a4608-3746-47ec-8faf-1379b28162c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b9a4608-3746-47ec-8faf-1379b28162c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3b9a4608-3746-47ec-8faf-1379b28162c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        id_code  diagnosis\n",
              "0  000c1434d8d7          2\n",
              "1  0024cdab0c1e          1\n",
              "2  0083ee8054ee          4\n",
              "3  00a8624548a9          2\n",
              "4  00b74780d31d          2"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = pd.read_csv(\n",
        "    LABEL_PATH + 'labels.csv'\n",
        ")\n",
        "\n",
        "print('Shape of labels:', y.shape)\n",
        "print('Unique diagnosis codes:', np.sort(y.diagnosis.unique()))\n",
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uMMBr7GqEfR"
      },
      "source": [
        "There are 200 training images corrresponding to 5 different diabetic retinopathy (DR) diagnosis codes:\n",
        "\n",
        "* No DR (0)\n",
        "* mild (1)\n",
        "* moderate (2)\n",
        "* severe (3)\n",
        "* proliferate DR (4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbTblbt-qEfR"
      },
      "source": [
        "Image inspection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "8BkNYbfZqEfR",
        "outputId": "9b36297f-7dc3-4af3-869b-f010574f581c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of images in data:\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ca14f2984ec3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sample of images in data:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     img = load_img(\n\u001b[1;32m      8\u001b[0m     IMAGE_PATH + img)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ],
      "source": [
        "# read image\n",
        "indx=0\n",
        "images = []\n",
        "\n",
        "print('Sample of images in data:')\n",
        "for idx, img in enumerate(os.listdir(IMAGE_PATH)):\n",
        "    img = load_img(\n",
        "    IMAGE_PATH + img)\n",
        "    images.append(img)\n",
        "\n",
        "nrows, ncols = 2,4 #print first 8 images\n",
        "f, axs = plt.subplots(nrows, ncols, figsize=(20,10))\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        indx = i*nrows+j\n",
        "        axs[i,j].imshow(images[indx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_dWKLZ7ut2jV",
        "outputId": "95e45fd5-d11c-4b02-eeb4-77fe5ae35eae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e1ad23b2-b446-4f75-a381-2093a44834ae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e1ad23b2-b446-4f75-a381-2093a44834ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Upload the ZIP file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Specify the name of the uploaded ZIP file\n",
        "zip_filename = next(iter(uploaded))\n",
        "\n",
        "# Directory where you want to extract the images\n",
        "extraction_path = '/content/extracted_images/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "# Extract the contents of the ZIP file\n",
        "with ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "# Path to the directory containing the extracted images\n",
        "image_path = os.path.join(extraction_path, 'CNN_images')\n",
        "\n",
        "# Read and display sample images\n",
        "indx = 0\n",
        "images = []\n",
        "\n",
        "print('Sample of images in data:')\n",
        "for idx, img_name in enumerate(os.listdir(image_path)):\n",
        "    img_path = os.path.join(image_path, img_name)\n",
        "    img = Image.open(img_path)\n",
        "    images.append(img)\n",
        "\n",
        "nrows, ncols = 2, 4  # print the first 8 images\n",
        "f, axs = plt.subplots(nrows, ncols, figsize=(20, 10))\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        indx = i * ncols + j\n",
        "        axs[i, j].imshow(images[indx])\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdi__AmBqEfR"
      },
      "source": [
        "Like any real-world data set, you can see that these images have different sizes and focus. Later, you will use image transformation and data augmentation techniques to remove some of this variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcsvaaqgqEfS"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5IRbg4XqEfS"
      },
      "source": [
        "The quality of the data determines how well a machine learning algorithm can learn. This section will apply some simple techniques to deal with class imbalance. We will then create training/validation/test datasets and perform image transformation and augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMJ4oFjgqEfS"
      },
      "source": [
        "---\n",
        "### Exercise 2 (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hcm1ut-eqEfS"
      },
      "source": [
        "1. Graph a histogram for the five classes of DR.\n",
        "2. Write down the percentage of the largest class in the written answer below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKcTfyBhqEfS"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfgcLRStqEfS"
      },
      "source": [
        "*Written answer*:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kASWSSccqEfS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRKFFTltqEfS"
      },
      "source": [
        "`Correct for data imbalance`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LvV-YO-qEfT"
      },
      "source": [
        "As you can see from the histogram above, our dataset is very imbalanced, which is common in healthcare, and it happens because some diseases are rare. The presence of imbalanced data hampers the detection of rare events as most classification methods implicitly assume a similar occurrence of classes and are designed to maximize the overall classification accuracy.\n",
        "\n",
        "We will correct for class imbalance in two ways:\n",
        "\n",
        "  * First, we will binarize the DR diagnosis as follows:\n",
        "     - 'no refer' are {No DR, mild}\n",
        "     - 'refer' are {Moderate, Severe, Proliferate}\n",
        "\n",
        "\n",
        "  * Second, we'll only take 80 random samples from the 'no refer' class and 80 from the 'refer' class.\n",
        "\n",
        "It is a crude method to deal with imbalanced data, but it will be good enough for this homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX_hwZQjqEfT"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "no_refer = y[y.diagnosis.isin((0,1))]\n",
        "refer = y[y.diagnosis.isin((2,3,4))]\n",
        "\n",
        "# randomly draw 80 images from each classes\n",
        "temp_no_refer = list(np.random.choice(\n",
        "    no_refer.id_code,\n",
        "    size=80,\n",
        "    replace=False\n",
        "))\n",
        "\n",
        "temp_refer = list(np.random.choice(\n",
        "    refer.id_code,\n",
        "    size=80,\n",
        "    replace=False\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INxW7X6hqEfT"
      },
      "source": [
        "Next, we will use the **preprocess_data_part1()** function defined below to generate lists of images and labels (`images_mini` and `y_mini`) based on the values in the temp_no_refer and temp_refer lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhI-7ObWqEfT"
      },
      "outputs": [],
      "source": [
        "def preprocess_data_part1(IMAGE_PATH, LABEL_PATH):\n",
        "    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n",
        "\n",
        "    Params:\n",
        "    -------\n",
        "    IMAGE_PATH (str): path to directory with images.\n",
        "    LABEL_PATH (str): path to directory with labels.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    images_mini  (np.ndarray): Images of shape (N, 224, 224, 3)\n",
        "    y_mini (np.ndarray): Labels of shape (N,)\n",
        "    \"\"\"\n",
        "    y_mini = []\n",
        "    images_mini = []\n",
        "\n",
        "    # create lists of images and labels `images_mini` and `y_mini`\n",
        "    # based on temp_no_refer and temp_refer selections\n",
        "    for idx, img in enumerate(os.listdir(IMAGE_PATH)):\n",
        "        # read labels\n",
        "        if img.split('.')[0] in temp_no_refer:\n",
        "                y_mini.append(0)\n",
        "        elif img.split('.')[0] in temp_refer:\n",
        "                y_mini.append(1)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "\n",
        "        # read image\n",
        "        img = load_img(\n",
        "            IMAGE_PATH + img,\n",
        "            target_size=(224, 224)\n",
        "        )\n",
        "\n",
        "        # transform image to array\n",
        "        img = img_to_array(img)\n",
        "\n",
        "        # append to images\n",
        "        images_mini.append(img)\n",
        "\n",
        "    # stack images and trasnform to array\n",
        "    images_mini = np.stack(images_mini)\n",
        "    y_mini = np.array(y_mini).flatten()\n",
        "\n",
        "    return images_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Wefae0qEfT"
      },
      "outputs": [],
      "source": [
        "# generate images and labels based on preprocess_data_part1() function\n",
        "images_mini, y_mini = preprocess_data_part1(IMAGE_PATH, LABEL_PATH)\n",
        "\n",
        "print(f\"images_mini shape {images_mini.shape}\")\n",
        "print(f\"y_mini shape {y_mini.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foQ-kp4TqEfT"
      },
      "source": [
        "`Create train/validation/test data` and ``perform image tranformation and augmentation``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I8cKgLQqEfU"
      },
      "source": [
        "The next step in the data preprocessing part is to split the data into training, validation, and test sets. Once we have these partitions, we will apply image transformation and augmentations.\n",
        "\n",
        "\n",
        "To give you an idea of what image transformation and augmentation do, let's see an example applied to the first image in our mini data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ_CcbrqqEfU"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(1234)\n",
        "\n",
        "fig = plt.figure(figsize=(14, 12))\n",
        "\n",
        "# pull first image from data\n",
        "image = images_mini[0]\n",
        "\n",
        "# plot original\n",
        "ax = fig.add_subplot(1, 5, 1)\n",
        "ax.imshow(array_to_img(image))\n",
        "ax.set_title('Original', size=15);\n",
        "\n",
        "# resize\n",
        "ax = fig.add_subplot(1, 5, 2)\n",
        "img_resize = tf.image.resize(image, size=(224, 224))\n",
        "ax.imshow(array_to_img(img_resize))\n",
        "ax.set_title('Step 1: Resize', size=15);\n",
        "\n",
        "\n",
        "# adjust brightness\n",
        "ax = fig.add_subplot(1, 5, 3)\n",
        "img_bright = tf.image.adjust_brightness(img_resize, 0.3)\n",
        "ax.imshow(array_to_img(img_bright))\n",
        "ax.set_title('Step 2: Brightness', size=15);\n",
        "\n",
        "\n",
        "# adjust contrast\n",
        "ax = fig.add_subplot(1, 5, 4)\n",
        "img_contrast = tf.image.adjust_contrast(img_bright, contrast_factor=3)\n",
        "ax.imshow(array_to_img(img_contrast))\n",
        "ax.set_title('Step 3: Contrast', size=15);\n",
        "\n",
        "\n",
        "# flip left right\n",
        "ax = fig.add_subplot(1, 5, 5)\n",
        "img_flip = tf.image.flip_left_right(img_contrast)\n",
        "ax.imshow(array_to_img(img_flip))\n",
        "ax.set_title('Step 4: Flip left right', size=15);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78wuxaJqEfU"
      },
      "source": [
        "Next, we will define and run the **preprocess_data_part2()** function to create:\n",
        "\n",
        "* train/validation/test sets with split (0.6,0.2,0.2)\n",
        "\n",
        "* image transformation and augmentation, as follows:\n",
        "\n",
        "<u>Applied on training, validation and test sets</u>:\n",
        "  - resize to IMAGE_SIZE =(224,224) using tf.image.resize()\n",
        "  - normalize all pixel values to the range (0,1)\n",
        "  \n",
        "<u>Applied on training set only</u> (note that this step will create additional/augmented copies of the training data):\n",
        "  - adjust brightness by adding DELTA=0.3 to the pixel values using tf.image.adjust_brighness()\n",
        "  - adjust contrast to CONTRAST_FACTOR=3 using tf.image.adjust_contrast()\n",
        "  - flip left right using tf.image.flip_left_right()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iMvquvoqEfU"
      },
      "source": [
        "The quantity and diversity of data gathered have a significant impact on the results of a CNN model. One can apply augmentations to artificially inflate the training dataset by warping the original data such that their label does not change. These augmentations can significantly improve learning results without collecting new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZd1sw_DqEfX"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (224, 224)\n",
        "CONTRAST_FACTOR = 3\n",
        "DELTA = 0.3\n",
        "\n",
        "def preprocess_data_part2(images, y, split=(0.6,0.2,0.2)):\n",
        "    \"\"\" Split data into train, validation and test sets; apply transformaions and augmentations\n",
        "\n",
        "    Params:\n",
        "    -------\n",
        "    images  (np.ndarray): Images of shape (N, 224, 224, 3)\n",
        "    y (np.ndarray): Labels of shape (N,)\n",
        "    split (tuple): 3 values summing to 1 defining split of train, validation and test sets\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train (np.ndarray): Train images of shape (N_train, 224, 224, 3)\n",
        "    y_train (np.ndarray): Train labels of shape (N_train,)\n",
        "    X_val (np.ndarray): Val images of shape (N_val, 224, 224, 3)\n",
        "    y_val (np.ndarray): Val labels of shape (N_val,)\n",
        "    X_test (np.ndarray): Test images of shape (N_test, 224, 224, 3)\n",
        "    y_test (np.ndarray): Test labels of shape (N_test,)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### create train/validation/test sets ###\n",
        "    #########################################\n",
        "    # NOTE: Each time you run this cell, you'll re-shuffle the data. The ordering will be the same due to the random seed generator\n",
        "    tf.random.set_seed(1234)\n",
        "    np.random.seed(1234)\n",
        "    shuffle = np.random.permutation(np.arange(images.shape[0]))\n",
        "    images, y = images[shuffle], y[shuffle]\n",
        "\n",
        "    splits = np.multiply(len(images_mini), split).astype(int)\n",
        "    X_train, X_val, X_test = np.split(images_mini, [splits[0], splits[0]+splits[1]])\n",
        "    y_train, y_val, y_test = np.split(y_mini, [splits[0], splits[0]+splits[1]])\n",
        "\n",
        "    ### image transformation on training, validation, and test data ###\n",
        "    ###################################################################\n",
        "    # image resize\n",
        "    X_train = tf.image.resize(X_train, size=IMAGE_SIZE)\n",
        "    X_val = tf.image.resize(X_val, size=IMAGE_SIZE)\n",
        "    X_test = tf.image.resize(X_test, size=IMAGE_SIZE)\n",
        "\n",
        "    # rescale image to [0,1], i.e., greyscale\n",
        "    X_train = X_train/255.0\n",
        "    X_val = X_val/255.0\n",
        "    X_test = X_test/255.0\n",
        "\n",
        "\n",
        "    ### image augmentation on training data ###\n",
        "    ###########################################\n",
        "    # adjust brightness\n",
        "    X_train_augm = tf.image.adjust_brightness(X_train, delta=DELTA)\n",
        "\n",
        "    # adjust contrast\n",
        "    X_train_augm = tf.image.adjust_contrast(X_train_augm, contrast_factor=CONTRAST_FACTOR)\n",
        "\n",
        "    # random flip\n",
        "    X_train_augm = tf.image.random_flip_left_right(X_train_augm)\n",
        "\n",
        "    # concatenate original X_train and augmented X_train data\n",
        "    X_train = tf.concat([X_train, X_train_augm],axis=0)\n",
        "\n",
        "    # concatenate y_train (note the label is preserved)\n",
        "    y_train_augm = y_train\n",
        "    y_train = tf.concat([y_train, y_train_augm],axis=0)\n",
        "\n",
        "    # shuffle X_train and y_train, i.e., shuffle two tensors in the same order\n",
        "    shuffle = tf.random.shuffle(tf.range(tf.shape(X_train)[0], dtype=tf.int32))\n",
        "    X_train = tf.gather(X_train, shuffle)\n",
        "    y_train = tf.gather(y_train, shuffle).numpy() #also transforms y_train to numpy array\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLuEOKiGqEfY"
      },
      "source": [
        "Let's sanity check our implementation of the preprocess_data_part2() function by printing the shape of (X,y) from train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01LmdNKqqEfY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocess_data_part2(\n",
        "    images_mini,\n",
        "    y_mini,\n",
        "    split=(0.6,0.2,0.2)\n",
        ")\n",
        "\n",
        "print(f\"X_train shape {X_train.shape}\")\n",
        "print(f\"y_train shape {y_train.shape}\")\n",
        "print(f\"X_val shape {X_val.shape}\")\n",
        "print(f\"y_val shape {y_val.shape}\")\n",
        "print(f\"X_test shape {X_test.shape}\")\n",
        "print(f\"y_test shape {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVCLyjEbqEfY"
      },
      "source": [
        "Let's also print out the first 8 train and validation examples with the label of each example as the title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TX0AxFOqEfY"
      },
      "outputs": [],
      "source": [
        "# print taining data\n",
        "print('Print training data examples:')\n",
        "nrows, ncols = 1,4 #print first 4 images\n",
        "f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n",
        "for i in range(ncols):\n",
        "    axs[i].imshow(array_to_img(X_train[i]))\n",
        "    axs[i].set(title=y_train[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJzivct0qEfY"
      },
      "outputs": [],
      "source": [
        "# print test data\n",
        "print('Print validation data examples:')\n",
        "nrows, ncols = 1,4 #print first 4 images\n",
        "f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n",
        "for i in range(ncols):\n",
        "    axs[i].imshow(array_to_img(X_val[i]))\n",
        "    axs[i].set(title=y_val[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-X6T9uDqEfZ"
      },
      "source": [
        "---\n",
        "### Exercise 3 (20 points)\n",
        "\n",
        "We've split the data into train, validation, and test sets. Explain the purpose of each of these.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08laSDZ-qEfZ"
      },
      "source": [
        "*Written answer*:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5T9My6nqEfZ"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1PVWknJqEfZ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC3BVzltqEfZ"
      },
      "source": [
        "Our objective is to build and train a CNN model to refer patients to doctors based on the severity of DR seen in these images. We are interested in exploring binary classification of 'no refer' and 'refer'.\n",
        "\n",
        "<u>The architecture of our CNN model is as follows</u>:\n",
        "\n",
        "1. the model receives input images of size 224 x 224 x 3 (the images have three color channels)\n",
        "2. the input data goes through two convolutional layers that have kernels of size 5 x 5\n",
        "3. the first convolution has 32 output feature maps, and the second one has 64\n",
        "4. each convolution layer is followed by a max-pooling layer (this will reduce the size of the feature maps)\n",
        "5. the last two layers of the model are fully connected with a droput layer in between\n",
        "\n",
        "For each convolution we use strides=(1,1) to preserve the dimension of the inputs in the resulting feature maps. For the pooling layers, we set strides=(2,2) to subsample the image and shrink the size of the output feature maps. For the dropout layer, we set the probability of dropping input units during training to 0.5.\n",
        "\n",
        "\n",
        "We will implement this architecture using TensorFlow Keras API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Y5j2JFqEfZ"
      },
      "source": [
        "``Build model``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BfvepSRqEfZ"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "\n",
        "# add first convolution layer to the model\n",
        "model.add(tf.keras.layers.Conv2D(\n",
        "    filters=32,\n",
        "    kernel_size=(5, 5),\n",
        "    strides=(1, 1),\n",
        "    padding='same',\n",
        "    data_format='channels_last',\n",
        "    name='conv_1',\n",
        "    activation='relu'))\n",
        "\n",
        "\n",
        "# add a max pooling layer with pool size (2,2) and strides of 2\n",
        "# (this will reduce the spatial dimensions by half)\n",
        "model.add(tf.keras.layers.MaxPool2D(\n",
        "    pool_size=(2, 2),\n",
        "    name='pool_1'))\n",
        "\n",
        "\n",
        "# add second convolutional layer\n",
        "model.add(tf.keras.layers.Conv2D(\n",
        "    filters=64,\n",
        "    kernel_size=(5, 5),\n",
        "    strides=(1, 1),\n",
        "    padding='same',\n",
        "    name='conv_2',\n",
        "    activation='relu'))\n",
        "\n",
        "# add second max pooling layer with pool size (2,2) and strides of 2\n",
        "# (this will further reduce the spatial dimensions by half)\n",
        "model.add(tf.keras.layers.MaxPool2D(\n",
        "    pool_size=(2, 2), name='pool_2')\n",
        ")\n",
        "\n",
        "\n",
        "# add a fully connected layer (need to flatten the output of the previous layers first)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=1024,\n",
        "    name='fc_1',\n",
        "    activation='relu'))\n",
        "\n",
        "# add dropout layer\n",
        "model.add(tf.keras.layers.Dropout(\n",
        "    rate=0.5))\n",
        "\n",
        "# add the last fully connected layer\n",
        "# this last layer sets the activation function to \"None\" in order to output the logits\n",
        "# note that passing activation = \"sigmoid\" will return class memembership probabilities but\n",
        "# in TensorFlow logits are prefered for numerical stability\n",
        "# set units=1 to get a single output unit (remember it's a binary classification problem)\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=1,\n",
        "    name='fc_2',\n",
        "    activation=None))\n",
        "\n",
        "\n",
        "# build model and print summary\n",
        "tf.random.set_seed(1)\n",
        "model.build(input_shape=(None, 224, 224, 3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZRgUOr2qEfa"
      },
      "source": [
        "``Compile model``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SoFC_U8qEfa"
      },
      "source": [
        "The next step is to compile the model. We have to decide on the type of optimizer, loss function, and metrics to compute.\n",
        "\n",
        "We will use the Adam optimizer for this assignment, the most popular gradient-based optimization algorithm. There are a few other choices, and you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). The loss (cost) function suitable for our binary classification model is [binary_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses). We will compute model accuracy on the training, validation and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMoWJJZKqEfa"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), #set from_ligits=True because our last layer does not apply sigmoid\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr2bojyEqEfa"
      },
      "source": [
        "``Fit model``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Kh8M5ZqEfa"
      },
      "source": [
        "Finally, we will fit the model with 10 epochs on the train set and validate on the validation set. The performance depends on the current starter hyperparameters such as learning rate and choice of optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxPJd24tqEfa"
      },
      "outputs": [],
      "source": [
        "# set random seed to get reproductible results\n",
        "# neural network algorithms are stochastic (e.g., due to random weight initialization); setting a random seed helps to get more stable results after each run\n",
        "# however, best way to deal with randomness is to repeat your experiment many times (30+) and use statistics to summarize the performance of the model\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_val, y_val)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vfnzY4qEfb"
      },
      "source": [
        "Next let's plot loss and accuracy for training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QSfuNWnqEfb"
      },
      "outputs": [],
      "source": [
        "hist = history.history\n",
        "x_arr = np.arange(len(hist['loss'])) + 1\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
        "ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n",
        "ax.legend(fontsize=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.set_ylabel('Loss', size=15)\n",
        "\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n",
        "ax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation acc.')\n",
        "ax.legend(fontsize=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.set_ylabel('Accuracy', size=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5mpmy_bqEfb"
      },
      "source": [
        "## Show what the CNN model is learning after each layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml_YBozQqEfb"
      },
      "source": [
        "We will pick one example from our training data to visualize our CNN model's learning after each layer. Below we print the original image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q74Gv0jRqEfb"
      },
      "outputs": [],
      "source": [
        "img_tensor = np.expand_dims(X_train[2], axis = 0);\n",
        "\n",
        "# Print image tensor shape\n",
        "print('Shape of image:', img_tensor.shape);\n",
        "\n",
        "# Print image\n",
        "plt.imshow(img_tensor[0]);\n",
        "plt.title('label:' + str(y_train[1]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tou7hBfqEfb"
      },
      "source": [
        "Next we print what the model learns after each layer-channel. It's important to pay attention to the shape of the output image to understand what each layer does to the original input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c41gI9mZqEfb"
      },
      "outputs": [],
      "source": [
        "# outputs of the first 4 layers, which include conv2D and max pooling layers\n",
        "layer_outputs = [layer.output for layer in model.layers[:4]]\n",
        "activation_model = models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "activations = activation_model.predict(img_tensor)\n",
        "\n",
        "# grab layer names\n",
        "layer_names = []\n",
        "for layer in model.layers[:4]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "# getting activations of each layer\n",
        "for idx, layer in enumerate(activations):\n",
        "    if idx in (0,1,2,3):\n",
        "        print('----------------')\n",
        "        print('Geeting activations of layer',  idx+1, ':', layer_names[idx])\n",
        "        activation = layer\n",
        "\n",
        "        # shape of layer activation\n",
        "        print('Images size is', activation.shape[1], 'x', activation.shape[2])\n",
        "        print('Number of channels is', activation.shape[3])\n",
        "\n",
        "        # print channels\n",
        "        print('Printing channels:')\n",
        "\n",
        "        # define nrows and ncols depending on number of channels\n",
        "        if idx in (0,1):\n",
        "            nrows, ncols = 4,8\n",
        "        if idx in (2,3):\n",
        "            nrows, ncols = 8,8\n",
        "\n",
        "        # plots\n",
        "        channel=0\n",
        "        if idx in (0,1):\n",
        "            f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n",
        "        if idx in (2,3):\n",
        "            f, axs = plt.subplots(nrows, ncols, figsize=(14,20))\n",
        "\n",
        "        for i in range(nrows):\n",
        "            for j in range(ncols):\n",
        "                if i==0 and j==0:\n",
        "                    channel=0\n",
        "                else:\n",
        "                    channel+=1\n",
        "\n",
        "                axs[i,j].matshow(activation[0,:, :, channel], cmap ='viridis')\n",
        "                axs[i,j].set(title=str(channel))\n",
        "                #axs[i,j].axis('off') # pay attention to the range of x and y axis\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LLH5aETqEfc"
      },
      "source": [
        "\n",
        "As you can see, initial layers are more interpretable and retain the majority of the features in the input image. As the level of the layer increases, features become less interpretable, they become more abstract and they identify features specific to the class leaving behind the general features of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CZnvej1qEfc"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEQA2YTyqEfc"
      },
      "source": [
        "Evaluation is one of the most important parts of machine learning as it helps us determine how good our trained model is in predicting unseen data.\n",
        "\n",
        "Notice that (`X_test`, and `y_test`) were not used in the training part. It would be very bad practice to evaluate the model on the test set, and then return and update the model based on those results (then the test set is acting like just another validation set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoa1NXRjqEfc"
      },
      "source": [
        "We will now use our test data to evaluate the performance (accuracy) of our CNN model on unseen data. Note that accuracy is the default metric if one compiles the model with the accuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPvLJywAqEfc"
      },
      "outputs": [],
      "source": [
        "test_results = model.evaluate(X_test, y_test)\n",
        "print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRIbxKhkqEfc"
      },
      "source": [
        "`get predictions results in the form of class-membership probabilities`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e9U7DHgqEfc"
      },
      "source": [
        "In the following figure, you can see all the images in the test data along with their ground truth (GT) labels and the predicted probabiliy that they belong to class 1, 'Refer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyANzmYKqEfc"
      },
      "outputs": [],
      "source": [
        "# transform logits to probabilities\n",
        "pred_logits = model.predict(X_test)\n",
        "probas = tf.sigmoid(pred_logits)\n",
        "probas = probas.numpy().flatten()*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnxg-KrIqEfd"
      },
      "outputs": [],
      "source": [
        "# plot test data and associated predicred\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for j, example in enumerate(X_test):\n",
        "    ax = fig.add_subplot(8,4, j+1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(array_to_img(example))\n",
        "    if y_test[j]==0:\n",
        "        label='no refer'\n",
        "    else:\n",
        "        label='refer'\n",
        "\n",
        "    ax.text(\n",
        "        0.5, -0.15,\n",
        "        'GT: {:s}\\nPr(refer)={:.0f}%'.format(label, probas[j]),\n",
        "        size=16,\n",
        "        color='grey',\n",
        "        horizontalalignment='center',\n",
        "        verticalalignment='center',\n",
        "        transform=ax.transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hRT3uPOqEfd"
      },
      "source": [
        "---\n",
        "### Exercise 4 (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbhflu7dqEfd"
      },
      "source": [
        "In this exercise, your task is to experiment with different hyperparameter values to see how sensitive the accuracy of our CNN model is.  \n",
        "\n",
        "1. In the table below, you will fill in `training and validation accuracy` results corresponding to the last epoch of the model fit.\n",
        "\n",
        "    You will change one hyperparameter value at a time, as follows:\n",
        "\n",
        "    * kernel size = 3 x 3\n",
        "    * strides = 2 x 2\n",
        "    * pool size = 3 x 3\n",
        "    * learning rate = 0.01\n",
        "    * optimizer = 'SGD'\n",
        "    * image augmentation applied on the training data: (brightness (delta) = 0.1, contrast = 2, flip = no)\n",
        "\n",
        "2. Finally, comment on the CNN model specification that you are most satisfied with based on the accuracy achieved on the training and validation sets.\n",
        "\n",
        "3. For this specification, what is the accuracy of the test dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWhfAgG5qEfd"
      },
      "source": [
        "*Written answer*:\n",
        "\n",
        "| Trining accuracy | Validation accuracy |  kernel size |  strides | pool size  |  learning rate | optimizer  | brightness (delta) |  contrast factor | flip_on_train  |\n",
        "|:-:                |:-:                  |:-:           |:-:       |:-:         |:-:             |:-:         |:-:                 |:-:               |:-:             |\n",
        "| 0.94              | 0.81                | 5,5          | 1,1      | 2,2        | 0.001          | Adam       | 0.3                | 3                | yes            |\n",
        "| ...               | ...                 | <font color=\"red\">3,3</font>     | 1,1      | 2,2        | 0.001          | Adam       | 0.3                | 3                | yes            |\n",
        "| ...               | ...                 | 5,5          | <font color=\"red\">2,2</font>  | 2,2        | 0.001          | Adam       | 0.3                | 3                | yes            |\n",
        "| ...               | ...                 | 5,5          | 1,1      | <font color=\"red\">3,3</font>   | 0.001          | Adam       | 0.3                | 3                | yes            |\n",
        "| ...               | ...                 | 5,5          | 1,1      | 2,2        | <font color=\"red\">0.01</font>       | Adam       | 0.3                | 3                | yes            |\n",
        "| ...               | ...                 | 5,5          | 1,1      | 2,2        | 0.001          |<font color=\"red\">SGD</font>     | 0.3                | 3                | yes            |\n",
        "| ...               | ...                 | 5,5          | 1,1      | 2,2        | 0.001          | Adam       | <font color=\"red\">0.1</font>            | 3                | yes            |\n",
        "| ...               | ...                 | 5,5          | 1,1      | 2,2        | 0.001          | Adam       | 0.3                | <font color=\"red\">2</font>            | yes            |\n",
        "| ...               | ...                 | 5,5          | 1,1      | 2,2        | 0.001          | Adam       | 0.3                | 3                | <font color=\"red\">no</font>         |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chwkbAFiqEfd"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}